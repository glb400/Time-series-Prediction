{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d07abf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.callbacks import EarlyStopping, LearningRateMonitor\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer, NaNLabelEncoder\n",
    "from pytorch_forecasting.metrics import MAE, SMAPE, PoissonLoss, QuantileLoss\n",
    "\n",
    "\n",
    "# set error params\n",
    "epsilon = 1e-6\n",
    "\n",
    "# read files and remove the rows with NaN value in each column\n",
    "df = pq.read_table('nanda_2years_top500.parquet').to_pandas()\n",
    "df = df.dropna()\n",
    "df\n",
    "\n",
    "# DATA PRE-PROCESS: CHANGE FORMAT\n",
    "# convert the data of the column 'plan_in_load_time' in pd.datetime format\n",
    "df['plan_in_load_time'] = pd.to_datetime(df['plan_in_load_time'])\n",
    "df['plan_in_load_time'] = df['plan_in_load_time'].apply(lambda x: x.date())\n",
    "df['plan_in_load_time'] = pd.to_datetime(df['plan_in_load_time'])\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d154bcf3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2023-11-30 00:00:00')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PRE-PROCESS: GET DATE RANGE\n",
    "# get MAX_DATE in df\n",
    "MAX_DATE = df['plan_in_load_time'].max()\n",
    "\n",
    "# !!! DUE TO WRONG TIMESTAMP !!!: \n",
    "# change the date of this row to '2023-11-30' in df but not in df_max_date\n",
    "df.loc[df['plan_in_load_time'] == MAX_DATE, 'plan_in_load_time'] = '2023-11-30'\n",
    "MAX_DATE = df['plan_in_load_time'].max()\n",
    "MAX_DATE\n",
    "\n",
    "\n",
    "MAX_DATES = []\n",
    "TEST_DATE = pd.to_datetime('2023-10-16')\n",
    "\n",
    "for t in range(30):\n",
    "    MAX_DATES.append(TEST_DATE + pd.Timedelta(days=t))\n",
    "\n",
    "MAX_DATES\n",
    "\n",
    "\n",
    "# get MIN_DATE in df\n",
    "MIN_DATE = df['plan_in_load_time'].min()\n",
    "MIN_DATE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1689f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA PRE-PROCESS: DATA GROUPBY\n",
    "# generate a new dataframe which is groupby df by the columns 'start_group_name' and 'end_group_name'\n",
    "df_gp = df.groupby(['start_group_name', 'end_group_name'])\n",
    "df_gp\n",
    "\n",
    "# CASE STUDY\n",
    "# transfer the groupby object to a list of small dataframes\n",
    "df_gp_list = [group for _, group in df_gp]\n",
    "# # get 0.7-quartile of the length of df_gp_list\n",
    "# np.quantile([len(i) for i in df_gp_list], 0.7)\n",
    "\n",
    "\n",
    "# transfer the groupby object to a list of small dataframes\n",
    "df_gp_list = []\n",
    "df_gp_dict = dict()\n",
    "\n",
    "tft_data = pd.DataFrame()\n",
    "\n",
    "for _, group in df_gp:\n",
    "    \n",
    "    # STEP1: get group name and group id\n",
    "    start_group_name, end_group_name = group['start_group_name'].values[0], group['end_group_name'].values[0]\n",
    "    print('{} - {}'.format(group['start_group_name'].values[0], group['end_group_name'].values[0]))\n",
    "    \n",
    "    # rename the column 'plan_in_load_time' of group to 'y'\n",
    "    group = group.rename(columns={'plan_in_load_time': 'date', 'is_success': 'count'})\n",
    "    \n",
    "    \n",
    "    # STEP2: get new dataframe\n",
    "    # get the column 'date' and 'count' of group\n",
    "    group['count'] = 1\n",
    "    group = group[['date', 'count']]\n",
    "    # group by 'date'\n",
    "    group = group.groupby('date').sum()\n",
    "    group = group[['count']]\n",
    "    \n",
    "    \n",
    "    # STEP3: adjust date of dataframe\n",
    "    # fill the missing days in group using the value 0\n",
    "    group.index = pd.to_datetime(group.index)\n",
    "    group = group.resample('D').sum()\n",
    "    group = group.fillna(0)\n",
    "    \n",
    "    # extend the data from MIN_DATE to MAX_DATE using the value 0 in column 'y'\n",
    "    group_max_date, group_min_date = group.index.max(), group.index.min()\n",
    "    group = pd.concat([group, pd.DataFrame({'count': [0] * (MAX_DATE - group_max_date).days}, index=pd.date_range(group_max_date + pd.Timedelta(days=1), MAX_DATE))])\n",
    "    group = pd.concat([group, pd.DataFrame({'count': [0] * (group_min_date - MIN_DATE).days}, index=pd.date_range(MIN_DATE, group_min_date - pd.Timedelta(days=1)))])\n",
    "    \n",
    "    # get back the date column\n",
    "    group.reset_index(inplace=True)\n",
    "    group = group.rename(columns={'index': 'date'})\n",
    "    \n",
    "    \n",
    "    # STEP4: ADD ID AND FEATURES\n",
    "    # add start_group_name+' '+end_group_name as id\n",
    "    group['idx'] = start_group_name + ' ' + end_group_name\n",
    "    # add group name\n",
    "    group['start_group_name'], group['end_group_name'] = start_group_name, end_group_name\n",
    "    \n",
    "    \n",
    "    # STEP5: add data to data structure\n",
    "    # add group to df_gp_list\n",
    "    df_gp_list.append(group)\n",
    "    # add group to df_gp_dict\n",
    "    df_gp_dict[start_group_name + ' ' + end_group_name] = group\n",
    "    # concat group to tft_data\n",
    "    tft_data = pd.concat([tft_data, group])\n",
    "    \n",
    "    # print(group)\n",
    "\n",
    "    \n",
    "print(df_gp_list)\n",
    "print('---')\n",
    "# number of groups\n",
    "print(len(df_gp_list))\n",
    "print('---')\n",
    "print(df_gp_dict)\n",
    "print('---')\n",
    "print(tft_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9ca46636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>count</th>\n",
       "      <th>idx</th>\n",
       "      <th>start_group_name</th>\n",
       "      <th>end_group_name</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>2.0</td>\n",
       "      <td>临沂 广州</td>\n",
       "      <td>临沂</td>\n",
       "      <td>广州</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>临沂 广州</td>\n",
       "      <td>临沂</td>\n",
       "      <td>广州</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-01-04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>临沂 广州</td>\n",
       "      <td>临沂</td>\n",
       "      <td>广州</td>\n",
       "      <td>6</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-01-05</td>\n",
       "      <td>12.0</td>\n",
       "      <td>临沂 广州</td>\n",
       "      <td>临沂</td>\n",
       "      <td>广州</td>\n",
       "      <td>7</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-01-06</td>\n",
       "      <td>4.0</td>\n",
       "      <td>临沂 广州</td>\n",
       "      <td>临沂</td>\n",
       "      <td>广州</td>\n",
       "      <td>8</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350995</th>\n",
       "      <td>2021-12-30</td>\n",
       "      <td>0.0</td>\n",
       "      <td>青岛 长沙</td>\n",
       "      <td>青岛</td>\n",
       "      <td>长沙</td>\n",
       "      <td>1</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350996</th>\n",
       "      <td>2021-12-31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>青岛 长沙</td>\n",
       "      <td>青岛</td>\n",
       "      <td>长沙</td>\n",
       "      <td>2</td>\n",
       "      <td>2021</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350997</th>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>青岛 长沙</td>\n",
       "      <td>青岛</td>\n",
       "      <td>长沙</td>\n",
       "      <td>3</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350998</th>\n",
       "      <td>2022-01-02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>青岛 长沙</td>\n",
       "      <td>青岛</td>\n",
       "      <td>长沙</td>\n",
       "      <td>4</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350999</th>\n",
       "      <td>2022-01-03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>青岛 长沙</td>\n",
       "      <td>青岛</td>\n",
       "      <td>长沙</td>\n",
       "      <td>5</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>351000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             date  count    idx start_group_name end_group_name  time_idx  \\\n",
       "0      2022-01-02    2.0  临沂 广州               临沂             广州         4   \n",
       "1      2022-01-03    0.0  临沂 广州               临沂             广州         5   \n",
       "2      2022-01-04    1.0  临沂 广州               临沂             广州         6   \n",
       "3      2022-01-05   12.0  临沂 广州               临沂             广州         7   \n",
       "4      2022-01-06    4.0  临沂 广州               临沂             广州         8   \n",
       "...           ...    ...    ...              ...            ...       ...   \n",
       "350995 2021-12-30    0.0  青岛 长沙               青岛             长沙         1   \n",
       "350996 2021-12-31    0.0  青岛 长沙               青岛             长沙         2   \n",
       "350997 2022-01-01    0.0  青岛 长沙               青岛             长沙         3   \n",
       "350998 2022-01-02    0.0  青岛 长沙               青岛             长沙         4   \n",
       "350999 2022-01-03    0.0  青岛 长沙               青岛             长沙         5   \n",
       "\n",
       "        year month day  \n",
       "0       2022     1   2  \n",
       "1       2022     1   3  \n",
       "2       2022     1   4  \n",
       "3       2022     1   5  \n",
       "4       2022     1   6  \n",
       "...      ...   ...  ..  \n",
       "350995  2021    12  30  \n",
       "350996  2021    12  31  \n",
       "350997  2022     1   1  \n",
       "350998  2022     1   2  \n",
       "350999  2022     1   3  \n",
       "\n",
       "[351000 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add time index\n",
    "tft_data['time_idx'] = (tft_data['date'] - tft_data['date'].min()).dt.days\n",
    "tft_data.reset_index(inplace=True, drop=True)\n",
    "tft_data\n",
    "\n",
    "# add date features\n",
    "tft_data['year'] = tft_data['date'].dt.year.astype(str)\n",
    "tft_data['month'] = tft_data['date'].dt.month.astype(str)\n",
    "tft_data['day'] = tft_data['date'].dt.day.astype(str)\n",
    "tft_data\n",
    "\n",
    "\n",
    "# CUT LATEST USELESS DATA\n",
    "tft_data = tft_data[lambda x: x.date <= (TEST_DATE + pd.Timedelta(days=30))]\n",
    "tft_data.reset_index(inplace=True)\n",
    "tft_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf558e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SMAPE_AVG = []\n",
    "\n",
    "# for t_id in range(7):\n",
    "    \n",
    "#     # PART1: PREPARE DATASET\n",
    "#     # TFT MODEL - Create dataset and dataloaders\n",
    "#     max_prediction_length = 1\n",
    "#     max_encoder_length = 14\n",
    "#     training_cutoff = tft_data[\"time_idx\"].max() - max_prediction_length\n",
    "    \n",
    "#     training = TimeSeriesDataSet(\n",
    "#         tft_data[lambda x: x.time_idx <= training_cutoff],\n",
    "#         time_idx='time_idx',\n",
    "#         target='count',\n",
    "#         group_ids=['idx'],\n",
    "#         min_encoder_length=max_encoder_length,\n",
    "#         max_encoder_length=max_encoder_length,\n",
    "#         min_prediction_length=max_prediction_length,\n",
    "#         max_prediction_length=max_prediction_length,\n",
    "#         static_categoricals=['start_group_name', 'end_group_name'],\n",
    "#         static_reals=[],\n",
    "#         time_varying_known_categoricals=['year', 'month', 'day'],\n",
    "#         variable_groups={},  # group of categorical variables can be treated as one variable\n",
    "#         time_varying_known_reals=['time_idx'],\n",
    "#         time_varying_unknown_categoricals=[],\n",
    "#         time_varying_unknown_reals=[\n",
    "#             'count',\n",
    "#         ],\n",
    "#         categorical_encoders={'year': NaNLabelEncoder().fit(tft_data.year), 'month': NaNLabelEncoder().fit(tft_data.month), \\\n",
    "#                               'day': NaNLabelEncoder().fit(tft_data.day)},\n",
    "#         add_relative_time_idx=True,\n",
    "#         add_target_scales=True,\n",
    "#         add_encoder_length=True\n",
    "#     )\n",
    "    \n",
    "#     # create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "#     # for each series\n",
    "#     validation = TimeSeriesDataSet.from_dataset(training, tft_data, predict=True, stop_randomization=True)\n",
    "    \n",
    "#     # create dataloaders for model\n",
    "#     batch_size = 50  # set this between 32 to 128\n",
    "#     train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "#     val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=8)\n",
    "    \n",
    "#     # PART2: TRAIN TFT MODEL\n",
    "#     # configure network and trainer\n",
    "#     early_stop_callback = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=False, mode='min')\n",
    "#     lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "#     logger = TensorBoardLogger('lightning_logs') \n",
    "    \n",
    "#     # build Trainer for TFT model\n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=1,\n",
    "#         min_epochs=1,\n",
    "#         accelerator='gpu',\n",
    "#         enable_model_summary=True,\n",
    "#         gradient_clip_val=0.01,\n",
    "#         limit_train_batches=1,  # coment in for training, running valiation every 30 batches\n",
    "#         # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "#         callbacks=[lr_logger, early_stop_callback],\n",
    "#         logger=logger,\n",
    "#     )\n",
    "    \n",
    "#     # build TFT model\n",
    "#     tft = TemporalFusionTransformer.from_dataset(\n",
    "#         training,\n",
    "#         learning_rate=0.01,\n",
    "#         hidden_size=8,\n",
    "#         attention_head_size=2,\n",
    "#         dropout=0.1,\n",
    "#         hidden_continuous_size=4,\n",
    "#         loss=QuantileLoss(quantiles=[0.5]),\n",
    "#         log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "#         optimizer='Ranger',\n",
    "#         reduce_on_plateau_patience=5,\n",
    "#     )\n",
    "#     print(f'Number of parameters in network: {tft.size()/1e3:.1f}k')\n",
    "    \n",
    "#     # fit network\n",
    "#     trainer.fit(\n",
    "#         tft,\n",
    "#         train_dataloaders=train_dataloader,\n",
    "#         val_dataloaders=val_dataloader,\n",
    "#     )\n",
    "    \n",
    "#     # PART3: EVALUATE TFT MODEL & GET SMAPE VALUE\n",
    "#     # load the best model according to the validation loss\n",
    "#     # (given that we use early stopping, this is not necessarily the last epoch)\n",
    "#     best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "#     best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "    \n",
    "#     # calcualte SMAPE on validation set\n",
    "#     predictions = best_tft.predict(val_dataloader, return_y=True)\n",
    "#     preds = torch.round(predictions.output)\n",
    "#     preds[preds < 0] = 0\n",
    "#     SMAPE_VAL = SMAPE()(preds, predictions.y)\n",
    "#     print('SMAPE_VALUE FOR DAY {} = {}'.format((7-t_id), SMAPE_VAL))\n",
    "#     SMAPE_AVG.append(SMAPE_VAL)\n",
    "    \n",
    "#     # PART4: CUT OFF THE DATA OF LAST DAY -> TO ITERATE AND TEST THE DAY BEFORE LAST DAY\n",
    "#     tft_data = tft_data[lambda x: x.time_idx < tft_data['time_idx'].max()]\n",
    "    \n",
    "    \n",
    "# SMAPE_AVG_VAL = sum(SMAPE_AVG)/7.0\n",
    "# print('FINAL SMAPE VALUE = {}'.format(SMAPE_AVG_VAL))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a608be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TUNING\n",
    "def write_to_file(file_path, content):\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(content)\n",
    "\n",
    "file_path = 'SMAPE_RESULTS_v3.txt'\n",
    "\n",
    "for tune_max_encoder_length in [21]:\n",
    "    for tune_epochs in [100]:\n",
    "        for tune_lr in [0.05]:\n",
    "            for tune_hidden_size in [32, 16, 8]:\n",
    "                for tune_hidden_continuous_size in [2, 4, 8, 16]:\n",
    "                    for tune_quantiles in [[0.5], [0.1, 0.5, 0.9]]:\n",
    "                        \n",
    "                        SMAPE_AVG = []\n",
    "                        \n",
    "                        for t_id in range(30):\n",
    "                            \n",
    "                            # PART1: PREPARE DATASET\n",
    "                            # TFT MODEL - Create dataset and dataloaders\n",
    "                            max_prediction_length = 1\n",
    "                            max_encoder_length = tune_max_encoder_length\n",
    "                            training_cutoff = tft_data[\"time_idx\"].max() - max_prediction_length\n",
    "                            \n",
    "                            training = TimeSeriesDataSet(\n",
    "                                tft_data[lambda x: x.time_idx <= training_cutoff],\n",
    "                                time_idx='time_idx',\n",
    "                                target='count',\n",
    "                                group_ids=['idx'],\n",
    "                                min_encoder_length=max_encoder_length,\n",
    "                                max_encoder_length=max_encoder_length,\n",
    "                                min_prediction_length=max_prediction_length,\n",
    "                                max_prediction_length=max_prediction_length,\n",
    "                                static_categoricals=['start_group_name', 'end_group_name'],\n",
    "                                static_reals=[],\n",
    "                                time_varying_known_categoricals=['year', 'month', 'day'],\n",
    "                                variable_groups={},  # group of categorical variables can be treated as one variable\n",
    "                                time_varying_known_reals=['time_idx'],\n",
    "                                time_varying_unknown_categoricals=[],\n",
    "                                time_varying_unknown_reals=[\n",
    "                                    'count',\n",
    "                                ],\n",
    "                                categorical_encoders={'year': NaNLabelEncoder().fit(tft_data.year), 'month': NaNLabelEncoder().fit(tft_data.month), \\\n",
    "                                                      'day': NaNLabelEncoder().fit(tft_data.day)},\n",
    "                                add_relative_time_idx=True,\n",
    "                                add_target_scales=True,\n",
    "                                add_encoder_length=True\n",
    "                            )\n",
    "                            \n",
    "                            # create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "                            # for each series\n",
    "                            validation = TimeSeriesDataSet.from_dataset(training, tft_data, predict=True, stop_randomization=True)\n",
    "                            \n",
    "                            # create dataloaders for model\n",
    "                            batch_size = 50  # set this between 32 to 128\n",
    "                            train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=8)\n",
    "                            val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 10, num_workers=8)\n",
    "                            \n",
    "                            # PART2: TRAIN TFT MODEL\n",
    "                            # configure network and trainer\n",
    "                            early_stop_callback = EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=10, verbose=False, mode='min')\n",
    "                            lr_logger = LearningRateMonitor()  # log the learning rate\n",
    "                            logger = TensorBoardLogger('lightning_logs') \n",
    "                            \n",
    "                            # build Trainer for TFT model\n",
    "                            trainer = pl.Trainer(\n",
    "                                max_epochs=tune_epochs,\n",
    "                                min_epochs=tune_epochs,\n",
    "                                accelerator='gpu',\n",
    "                                enable_model_summary=True,\n",
    "                                gradient_clip_val=0.01,\n",
    "                                limit_train_batches=tune_epochs,  # coment in for training, running valiation every 30 batches\n",
    "                                # fast_dev_run=True,  # comment in to check that networkor dataset has no serious bugs\n",
    "                                callbacks=[lr_logger, early_stop_callback],\n",
    "                                logger=logger,\n",
    "                            )\n",
    "                            \n",
    "                            # build TFT model\n",
    "                            tft = TemporalFusionTransformer.from_dataset(\n",
    "                                training,\n",
    "                                learning_rate=tune_lr,\n",
    "                                hidden_size=tune_hidden_size,\n",
    "                                attention_head_size=2,\n",
    "                                dropout=0.1,\n",
    "                                hidden_continuous_size=tune_hidden_continuous_size,\n",
    "                                loss=QuantileLoss(quantiles=tune_quantiles),\n",
    "                                log_interval=10,  # uncomment for learning rate finder and otherwise, e.g. to 10 for logging every 10 batches\n",
    "                                optimizer='Ranger',\n",
    "                                reduce_on_plateau_patience=5,\n",
    "                            )\n",
    "                            print(f'Number of parameters in network: {tft.size()/1e3:.1f}k')\n",
    "                            \n",
    "                            # fit network\n",
    "                            trainer.fit(\n",
    "                                tft,\n",
    "                                train_dataloaders=train_dataloader,\n",
    "                                val_dataloaders=val_dataloader,\n",
    "                            )\n",
    "                            \n",
    "                            # PART3: EVALUATE TFT MODEL & GET SMAPE VALUE\n",
    "                            # load the best model according to the validation loss\n",
    "                            # (given that we use early stopping, this is not necessarily the last epoch)\n",
    "                            best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "                            best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "                            \n",
    "                            # calcualte SMAPE on validation set\n",
    "                            predictions = best_tft.predict(val_dataloader, return_y=True)\n",
    "                            preds = torch.round(predictions.output)\n",
    "                            preds[preds < 0] = 0\n",
    "                            SMAPE_VAL = SMAPE()(preds, predictions.y)\n",
    "                            print('SMAPE_VALUE FOR DAY {} = {}'.format((30-t_id), SMAPE_VAL))\n",
    "                            SMAPE_AVG.append(SMAPE_VAL)\n",
    "                            \n",
    "                            # PART4: CUT OFF THE DATA OF LAST DAY -> TO ITERATE AND TEST THE DAY BEFORE LAST DAY\n",
    "                            tft_data = tft_data[lambda x: x.time_idx < tft_data['time_idx'].max()]\n",
    "                            \n",
    "                            \n",
    "                        SMAPE_AVG_VAL = sum(SMAPE_AVG)/30.0\n",
    "                        print('FINAL SMAPE VALUE = {}'.format(SMAPE_AVG_VAL))\n",
    "                        content = 'max_encoder_length = {}, epochs = {}, lr = {}, hidden_size = {}, hidden_continuous_size = {}, quantiles = {}, SMAPE VALUE = {}, SMAPE LIST = {}\\n'.format(tune_max_encoder_length, tune_epochs, tune_lr, tune_hidden_size, tune_hidden_continuous_size, tune_quantiles, SMAPE_AVG_VAL, SMAPE_AVG)\n",
    "                        write_to_file(file_path, content)\n",
    "                        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945a7aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw predictions are a dictionary from which all kind of information including quantiles can be extracted\n",
    "raw_predictions = best_tft.predict(val_dataloader, mode='raw', return_x=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15abf77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINALLY GENERATE OUTPUT FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8631ca72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
